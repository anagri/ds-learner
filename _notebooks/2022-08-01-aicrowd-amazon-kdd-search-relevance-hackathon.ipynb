{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976be626",
   "metadata": {},
   "source": [
    "# AICrowd Amazon KDD Search Relevance Hackathon Experience\n",
    "> Sharing experience of my participation in Amazon KDD Cup Challenge\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [nlp hackathon]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e85525",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "[Amazon KDD Cup Challenge](https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search#timeline) is a hackathon hosted by [AICrowd](https://www.aicrowd.com/) platform. The hackathon involves improving the customer experience and engagement by improving the search relevance significantly using the cutting edge research in the fields of Search, NLP and Embeddings.\n",
    "\n",
    "The dataset included > 130k queries, > 1M product catalogs, > 2.6M judgements, with data distributed across us, es and jp locales. It is one of the largest multi-lingual search relevance based dataset I have seen in hackathons.\n",
    "\n",
    "The hackthon was kicked off on March 15th 2002, with the final submission on July 20th 2022.\n",
    "\n",
    "The hackathon involves 3 separate tasks of improving the identification of products as per ESCI (Exact, Substitute, Complement, Irrelevant) categories.\n",
    "\n",
    "## Task 1 - Query Product Ranking\n",
    "\n",
    "Given a search query, rank potential product matches based on their relevance\n",
    "\n",
    "\n",
    "## Task 2 - Multiclass Product Classification\n",
    "\n",
    "Given a asearch query and product pair, classify the pair among as ESCI category\n",
    "\n",
    "## Task 3 - Product Subtitute Identification\n",
    "\n",
    "Given a search query and product pair, predict if product partially fulfills the search criteria and can be used as functional substitute\n",
    "\n",
    "Apart from the cash prizes, the winners of the KDD Cup challenge will also get a chance to showcase their winning approach as paper in SIGKDD Conference 2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc0b6b",
   "metadata": {},
   "source": [
    "**I only attempted task 1, as I joined the competition very late and wanted to focus on getting a better rank in one task, than baseline in all 3. I will share my experience solving for task 1 here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8adf1",
   "metadata": {},
   "source": [
    "# Phase 1: Baseline\n",
    "\n",
    "The Amazon KDD Challenge also provided a [baseline code](https://github.com/amazon-research/esci-code) and accompanying paper detailing the baseline approach. \n",
    "\n",
    "For task 1 - query product ranking, the baseline approach used MS MARCO Cross Encoders for US, and multilingua MPNet for ES and JP locales.\n",
    "\n",
    "I used this baseline as a starting point, and iteratively refined my approach to improve on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b34f5f",
   "metadata": {},
   "source": [
    "## Step 1: Retrofit the Baseline into AICrowd Submission format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc12f745",
   "metadata": {},
   "source": [
    "For submission, we had to fork a baseline repository on AICrowd Gitlab. Tags pushed to this repository with name \"submission-\\*\" were picked by the CI server and executed to produce predictions.\n",
    "\n",
    "The template repo provided with `run.py`, which had a dummy implementation for prediction. Just to test, end-to-end was working fine, checked in the dummy implementation, and pushed the changes. To trigger the CI and evaluation step, created git tag `submission-initial-version` and pushed the tag to remote.\n",
    "\n",
    "As expected, the tag was picked by the CI process, and after a bit of wait, the server gave me my first result:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.7486\n",
    "```\n",
    "\n",
    "With this, I had a spot on the leaderboard as well ‚ò∫Ô∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753572c",
   "metadata": {},
   "source": [
    "## Step 2: Training the Model and making the first real submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319474b",
   "metadata": {},
   "source": [
    "Suprisingly, the baseline solutions provided on github, and baseline repositories forked on gitlab differed quite a lot in their structure. So to get started, one had to really understand the esci-baseline, copy over the training and predictions components, and plug-in with the forked aicrowd gitlab baseline.\n",
    "\n",
    "IMHO, much time could have been saved if there was consistency between both of these repos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ace85f",
   "metadata": {},
   "source": [
    "As a golden rule of any Machine Learning project, my first goal was to get a baseline submission through, and then iteratively tweek my approach to better result.\n",
    "\n",
    "\n",
    "Once, I refactored and merged the 2 baseline repos provided in a working condition on my machine, the next was to get my model trained.\n",
    "\n",
    "I used [lambdalabs.com](https://lambdalabs.com/) as my cloud GPU provider as its pay-as-you-use approach, and array of GPU selections satisfied my requirements.\n",
    "\n",
    "I pushed my changes to AICrowd gitlab, and cloned it back on the lambdalabs persistent storage disk. I also setup a virtual env on the persistent disk so as not to set it up everytime I spin my VM.\n",
    "\n",
    "**Basic profiling showed me the baseline approach was not optimized or utilized multiple GPUs, so I stick to low-cost GPUs (A6000, $0.80/hr) for training the model.**\n",
    "\n",
    "Once my model was trained, I tested a dummy evaluation cycle on the VM itself, and it produced the `results.csv`. This gave me confidence that all the pieces of code are working as expected. Then, I checked my model files using `git-lfs` and pushed the code to AICrowd repos. To trigger the CI step, I created the git tag `submission-initial-version` and pushed to remote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8429df7",
   "metadata": {},
   "source": [
    "Once my model was trained, I tested a dummy evaluation cycle on the VM itself, and it produced the `results.csv`. This gave me confidence that all the pieces of code are working as expected. Then, I checked my large model files using `git-lfs` and pushed the code to AICrowd repos. To trigger the CI step, I created the git tag `submission-baseline` and pushed to remote.\n",
    "\n",
    "This tag failed, as on the CI setup, the products-catalogue file was missing. This took sometime to debug as even the git logs were not available. I raised the issue on discussion board and discord. I was advised to check in the catalogue file, so I did.\n",
    "\n",
    "After some attempts, the tag went through and produced result based on the model:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.8505\n",
    "```\n",
    "\n",
    "An increase of ~0.1 from the random prediction version. Good start but not as much as I was hoping for. With this my ranking improved significantly on leaderboard and was under 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397fd68",
   "metadata": {},
   "source": [
    "# Phase 2: Improving the Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005eb96b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773f14c",
   "metadata": {},
   "source": [
    "I started off with basic EDA on the data. Few of the results of data exploration that I later incorporated in my model:\n",
    "\n",
    "1. There were other fields in the data which had rich information. In the baseline, we were only using the title field, so there was a venue to improve prediction including other fields as well\n",
    "2. There were some missing fields, but overall, there were only few rows where all the fields were missing\n",
    "3. The non-alphanum/symbols in the data were of significant context as they represented some domain information. E.g. `#` was extensively used to denote the size or model of the object like lipstick, pen etc. So in general, clean up of data was limited to removing unnecessary spaces, and lowercasing for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808abafb",
   "metadata": {},
   "source": [
    "## Iteration 1: Including more fields in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f81825",
   "metadata": {},
   "source": [
    "As next step, I included all the fields after doing clean up in the model training. The code looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-block-start\n",
    "\n",
    "# cleaning title\n",
    "df_p[col_title_clean] = df_p[col_product_title] \\\n",
    "    .str.lower() \\\n",
    "    .str.replace(replace_pattern, \" \", regex=True) \\\n",
    "    .str.replace(\"\\s{2,}\", \" \", regex=True) \\\n",
    "    .str.strip()\n",
    "\n",
    "# ...\n",
    "\n",
    "# merging all fields under a single field for training\n",
    "df_p[col_text_all] = df_p.apply(\n",
    "    lambda df: '<id> ' + df[col_product_id] + ' <id>' + \\\n",
    "               ' <title> ' + df[col_title_clean] + ' <title>' + \\\n",
    "               ' <brand> ' + df[col_product_brand] + ' <brand>' + \\\n",
    "               ' <color> ' + df[col_color_clean] + ' <color>' + \\\n",
    "               ' <desc> ' + df[col_description_clean] + ' <desc>' + \\\n",
    "               ' <bullet> ' + df[col_bullet_clean] + ' <bullet>', axis=1)\n",
    "\n",
    "# code-block-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d6d9e",
   "metadata": {},
   "source": [
    "I was hoping this would give significant boost to my result, but I got the results below for above changes:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.8404\n",
    "```\n",
    "\n",
    "This was a ~0.01 drop to my previous result where I was only including the title field. There was something wrong with my approach that I needed to debug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b30e01",
   "metadata": {},
   "source": [
    "## Iteration 2: Unifying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef37714",
   "metadata": {},
   "source": [
    "The baseline was having different models and approaches based on locale. So in all, it had 3 different models for each of the 3-locales - US, ES and JP. This was making it hard to optimize as you need to take multiple approaches based on locale. I thought of testing if having a single multi-lingual model and a uniform approach would provide a comparative result, and then take steps to optimize my results.\n",
    "\n",
    "I chose `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1` as it was the same cross-encoder model, but having multi-lingual support. Luckily, it also had support for ES and JP locales.\n",
    "\n",
    "So I ditched the esci-baseline approach of 3 different models, and trained all of my data on this single model and got the result:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.8813\n",
    "```\n",
    "\n",
    "Awesome, not only I got better result from this approach, the boost was a significant ~0.03 over my last best result, and ~0.04 over my approach of including more fields for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e5dab",
   "metadata": {},
   "source": [
    "## Iteration 3: Fixing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d572",
   "metadata": {},
   "source": [
    "During the EDA, I saw a pattern in the training data provided. There were rows in the data where query_id was same, but query_text differed, and then there were rows, where query_text was same but query_id were different. A hunch was that the data was tampered to create randomness.\n",
    "\n",
    "I decided to fix this issue and see if the model performance improved. I grouped the rows by query_text, and then reset the query_id to match them. This was because the ESCI label matched more with query_text than with query_id.\n",
    "\n",
    "I was hoping some improvement in model performance, but the result remained same:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.8813\n",
    "```\n",
    "\n",
    "No worries, on to next iteration and keeping fingers crossed for result improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a820a",
   "metadata": {},
   "source": [
    "## Iteration 4: Deeper Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3acb7",
   "metadata": {},
   "source": [
    "After exploring multiple venues and not finding any promising leads on improving the performance, last arrow in my quiver was to train the model deeper. Earlier I was training the model for a single epoch, I increased it to 3 to see if it improved the performance. The result I got was:\n",
    "\n",
    "```\n",
    "üèÜ Scores\n",
    "NDCG Score : 0.8847\n",
    "```\n",
    "\n",
    "An improvement of ~0.003. Though this helped with ranking, but also took thrice as longer to train and as much the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ccb1a",
   "metadata": {},
   "source": [
    "## Submission Pile-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362f0ec",
   "metadata": {},
   "source": [
    "As the hackathon came to a close, there was a rush of submission in the last few days. This resulted in wait time as much as 18hrs. This was a bit demotivating in the last stretch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ef9cc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976c480",
   "metadata": {},
   "source": [
    "In the end, I was happy with the hard-work I put up in this hackathon. I learned a lot about NLP, Sentence Vectors etc., and was able to put in my learning in a very close-to-real- business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a5eca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e494c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazonkdd",
   "language": "python",
   "name": "amazonkdd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
