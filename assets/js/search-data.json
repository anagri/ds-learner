{
  
    
        "post0": {
            "title": "AICrowd Amazon KDD Search Relevance Hackathon Experience",
            "content": "About . Amazon KDD Cup Challenge is a hackathon hosted by AICrowd platform. The hackathon involves improving the customer experience and engagement by improving the search relevance significantly using the cutting edge research in the fields of Search, NLP and Embeddings. . The dataset included &gt; 130k queries, &gt; 1M product catalogs, &gt; 2.6M judgements, with data distributed across us, es and jp locales. It is one of the largest multi-lingual search relevance based dataset I have seen in hackathons. . The hackthon was kicked off on March 15th 2002, with the final submission on July 20th 2022. . The hackathon involves 3 separate tasks of improving the identification of products as per ESCI (Exact, Substitute, Complement, Irrelevant) categories. . Task 1 - Query Product Ranking . Given a search query, rank potential product matches based on their relevance . Task 2 - Multiclass Product Classification . Given a asearch query and product pair, classify the pair among as ESCI category . Task 3 - Product Subtitute Identification . Given a search query and product pair, predict if product partially fulfills the search criteria and can be used as functional substitute . Apart from the cash prizes, the winners of the KDD Cup challenge will also get a chance to showcase their winning approach as paper in SIGKDD Conference 2022. . I only attempted task 1, as I joined the competition very late and wanted to focus on getting a better rank in one task, than baseline in all 3. I will share my experience solving for task 1 here. . Phase 1: Baseline . The Amazon KDD Challenge also provided a baseline code and accompanying paper detailing the baseline approach. . For task 1 - query product ranking, the baseline approach used MS MARCO Cross Encoders for US, and multilingua MPNet for ES and JP locales. . I used this baseline as a starting point, and iteratively refined my approach to improve on the results. . Step 1: Retrofit the Baseline into AICrowd Submission format . For submission, we had to fork a baseline repository on AICrowd Gitlab. Tags pushed to this repository with name &quot;submission-*&quot; were picked by the CI server and executed to produce predictions. . The template repo provided with run.py, which had a dummy implementation for prediction. Just to test, end-to-end was working fine, checked in the dummy implementation, and pushed the changes. To trigger the CI and evaluation step, created git tag submission-initial-version and pushed the tag to remote. . As expected, the tag was picked by the CI process, and after a bit of wait, the server gave me my first result: . üèÜ Scores NDCG Score : 0.7486 . With this, I had a spot on the leaderboard as well ‚ò∫Ô∏è. . Step 2: Training the Model and making the first real submission . Suprisingly, the baseline solutions provided on github, and baseline repositories forked on gitlab differed quite a lot in their structure. So to get started, one had to really understand the esci-baseline, copy over the training and predictions components, and plug-in with the forked aicrowd gitlab baseline. . IMHO, much time could have been saved if there was consistency between both of these repos. . As a golden rule of any Machine Learning project, my first goal was to get a baseline submission through, and then iteratively tweek my approach to better result. . Once, I refactored and merged the 2 baseline repos provided in a working condition on my machine, the next was to get my model trained. . I used lambdalabs.com as my cloud GPU provider as its pay-as-you-use approach, and array of GPU selections satisfied my requirements. . I pushed my changes to AICrowd gitlab, and cloned it back on the lambdalabs persistent storage disk. I also setup a virtual env on the persistent disk so as not to set it up everytime I spin my VM. . Basic profiling showed me the baseline approach was not optimized or utilized multiple GPUs, so I stick to low-cost GPUs (A6000, $0.80/hr) for training the model. . Once my model was trained, I tested a dummy evaluation cycle on the VM itself, and it produced the results.csv. This gave me confidence that all the pieces of code are working as expected. Then, I checked my model files using git-lfs and pushed the code to AICrowd repos. To trigger the CI step, I created the git tag submission-initial-version and pushed to remote. . Once my model was trained, I tested a dummy evaluation cycle on the VM itself, and it produced the results.csv. This gave me confidence that all the pieces of code are working as expected. Then, I checked my large model files using git-lfs and pushed the code to AICrowd repos. To trigger the CI step, I created the git tag submission-baseline and pushed to remote. . This tag failed, as on the CI setup, the products-catalogue file was missing. This took sometime to debug as even the git logs were not available. I raised the issue on discussion board and discord. I was advised to check in the catalogue file, so I did. . After some attempts, the tag went through and produced result based on the model: . üèÜ Scores NDCG Score : 0.8505 . An increase of ~0.1 from the random prediction version. Good start but not as much as I was hoping for. With this my ranking improved significantly on leaderboard and was under 20. . Phase 2: Improving the Baseline . Exploratory Data Analysis . I started off with basic EDA on the data. Few of the results of data exploration that I later incorporated in my model: . There were other fields in the data which had rich information. In the baseline, we were only using the title field, so there was a venue to improve prediction including other fields as well | There were some missing fields, but overall, there were only few rows where all the fields were missing | The non-alphanum/symbols in the data were of significant context as they represented some domain information. E.g. # was extensively used to denote the size or model of the object like lipstick, pen etc. So in general, clean up of data was limited to removing unnecessary spaces, and lowercasing for consistency | Iteration 1: Including more fields in model training . As next step, I included all the fields after doing clean up in the model training. The code looked like: . # cleaning title df_p[col_title_clean] = df_p[col_product_title] .str.lower() .str.replace(replace_pattern, &quot; &quot;, regex=True) .str.replace(&quot; s{2,}&quot;, &quot; &quot;, regex=True) .str.strip() # ... # merging all fields under a single field for training df_p[col_text_all] = df_p.apply( lambda df: &#39;&lt;id&gt; &#39; + df[col_product_id] + &#39; &lt;id&gt;&#39; + &#39; &lt;title&gt; &#39; + df[col_title_clean] + &#39; &lt;title&gt;&#39; + &#39; &lt;brand&gt; &#39; + df[col_product_brand] + &#39; &lt;brand&gt;&#39; + &#39; &lt;color&gt; &#39; + df[col_color_clean] + &#39; &lt;color&gt;&#39; + &#39; &lt;desc&gt; &#39; + df[col_description_clean] + &#39; &lt;desc&gt;&#39; + &#39; &lt;bullet&gt; &#39; + df[col_bullet_clean] + &#39; &lt;bullet&gt;&#39;, axis=1) # code-block-end . I was hoping this would give significant boost to my result, but I got the results below for above changes: . üèÜ Scores NDCG Score : 0.8404 . This was a ~0.01 drop to my previous result where I was only including the title field. There was something wrong with my approach that I needed to debug. . Iteration 2: Unifying the Model . The baseline was having different models and approaches based on locale. So in all, it had 3 different models for each of the 3-locales - US, ES and JP. This was making it hard to optimize as you need to take multiple approaches based on locale. I thought of testing if having a single multi-lingual model and a uniform approach would provide a comparative result, and then take steps to optimize my results. . I chose cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 as it was the same cross-encoder model, but having multi-lingual support. Luckily, it also had support for ES and JP locales. . So I ditched the esci-baseline approach of 3 different models, and trained all of my data on this single model and got the result: . üèÜ Scores NDCG Score : 0.8813 . Awesome, not only I got better result from this approach, the boost was a significant ~0.03 over my last best result, and ~0.04 over my approach of including more fields for training. . Iteration 3: Fixing Training Data . During the EDA, I saw a pattern in the training data provided. There were rows in the data where query_id was same, but query_text differed, and then there were rows, where query_text was same but query_id were different. A hunch was that the data was tampered to create randomness. . I decided to fix this issue and see if the model performance improved. I grouped the rows by query_text, and then reset the query_id to match them. This was because the ESCI label matched more with query_text than with query_id. . I was hoping some improvement in model performance, but the result remained same: . üèÜ Scores NDCG Score : 0.8813 . No worries, on to next iteration and keeping fingers crossed for result improvement. . Iteration 4: Deeper Training . After exploring multiple venues and not finding any promising leads on improving the performance, last arrow in my quiver was to train the model deeper. Earlier I was training the model for a single epoch, I increased it to 3 to see if it improved the performance. The result I got was: . üèÜ Scores NDCG Score : 0.8847 . An improvement of ~0.003. Though this helped with ranking, but also took thrice as longer to train and as much the cost. . Submission Pile-Up . As the hackathon came to a close, there was a rush of submission in the last few days. This resulted in wait time as much as 18hrs. This was a bit demotivating in the last stretch. . Conclusion . In the end, I was happy with the hard-work I put up in this hackathon. I learned a lot about NLP, Sentence Vectors etc., and was able to put in my learning in a very close-to-real- business problem .",
            "url": "https://anagri.github.io/ds-learner/nlp%20hackathon/2022/08/01/aicrowd-amazon-kdd-search-relevance-hackathon.html",
            "relUrl": "/nlp%20hackathon/2022/08/01/aicrowd-amazon-kdd-search-relevance-hackathon.html",
            "date": " ‚Ä¢ Aug 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://anagri.github.io/ds-learner/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://anagri.github.io/ds-learner/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello, . Welcome to blog of my Data Science journey. . My name is Amir, you can find my professional experience details on my linkedin profile. . I have a passion for data science, and have been on a self learning journey for over a decade. . I started off my data science journey in 2011 with completing ai-class.com (now known as udacity) and ml-class.com (now known as coursera.org) . I never transitioned into a full time data scientist, as I enjoy Software and Application Development üòÇ. . Since last year (jan 2021), I have deep dived into NLP and hoping to build some creative apps utilizing my wide experience in varied technology fields. . Feel free to connect and drop me a message on LinkedIn. .",
          "url": "https://anagri.github.io/ds-learner/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://anagri.github.io/ds-learner/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}